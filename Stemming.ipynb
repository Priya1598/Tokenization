{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621df7b7",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. stemming is important in natural language understanding (NLU) and Natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428658d",
   "metadata": {},
   "source": [
    "### RegexpStemmer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6541d35",
   "metadata": {},
   "source": [
    "#### NLTK has regexpstemmer class with the help of which we can easily implement regular expression stemmer algorithms. It basically takes a single regular expression and remove prefix or suffix that matches the expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e149f9b",
   "metadata": {},
   "source": [
    "### Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3602fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b470baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e04910",
   "metadata": {},
   "outputs": [],
   "source": [
    "Words= ['eating','eats','eaten','writing','programs','programming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7435e57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "writing------>write\n",
      "programs------>program\n",
      "programming------>program\n"
     ]
    }
   ],
   "source": [
    "for word in Words:\n",
    "    print(word+\"------>\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2499c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912593be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ed334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e17602",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stem=RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060e16e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbd159b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem(\"ingeating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b72cd",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5b41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d2b3e",
   "metadata": {},
   "source": [
    "#### snowball stemmer gives better accuracy than the porter stemmer, Lemmetization gives better accuracy than the stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "160c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmm= SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c72377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eaten\n",
      "writing------>write\n",
      "programs------>program\n",
      "programming------>program\n"
     ]
    }
   ],
   "source": [
    "for word in Words:\n",
    "    (print(word+\"------>\"+stemmm.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3706e422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97f65183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'goe', 'go')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmm.stem(\"Fairly\"),stemmm.stem(\"goes\"),stemmm.stem(\"going\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c3d6f",
   "metadata": {},
   "source": [
    "### Wordnet Lemmatizer\n",
    "\n",
    "Lemmatization technique is like stemming.The output we will get after lemmatization is called 'lemma',which is a root word rather than root stem, the output of stemming. After lemmatization, we will be  getting a valid word that means the same thing.\n",
    "\n",
    "NLTK provides Wordnet lemmatizer class which is a thin wrapper  around the wordnet corpus. This class uses Morphy() function to the Wordnet Corpus Reader class to find a lemma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7d8cb",
   "metadata": {},
   "source": [
    "##### Uses Cases ---> Q & A, Chatbots, Text Summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cac28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb87dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7de4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lemma.lemmatize('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d442d08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goes'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lemma.lemmatize('goes',pos='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc2e01f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "programs------>program\n",
      "programming------>program\n"
     ]
    }
   ],
   "source": [
    "for word in Words:\n",
    "    (print(word+\"------>\"+Lemma.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73547a76",
   "metadata": {},
   "source": [
    "Lemmatizer takes more time than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b6576",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5adbb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraph = '''In 1991, he took over as chairman from JRD Tata. Under his chairmanship, the company saw mergers like Land Rover Jaguar's merger with Tata Motors, Corus's merger with Tata Steel, Tetley's merger with Tata Tea, Brunner Mond, General Chemical Industrial Products and Daewoo. He launched the Tata Nano, India’s most affordable car at Rs. 1 lakh.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8961b06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 1991, he took over as chairman from JRD Tata. Under his chairmanship, the company saw mergers like Land Rover Jaguar's merger with Tata Motors, Corus's merger with Tata Steel, Tetley's merger with Tata Tea, Brunner Mond, General Chemical Industrial Products and Daewoo. He launched the Tata Nano, India’s most affordable car at Rs. 1 lakh.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac512c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c2a5ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "755203c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('German')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633c9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05dd6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence=nltk.sent_tokenize(Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "217e454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In 1991, he took over as chairman from JRD Tata.',\n",
       " \"Under his chairmanship, the company saw mergers like Land Rover Jaguar's merger with Tata Motors, Corus's merger with Tata Steel, Tetley's merger with Tata Tea, Brunner Mond, General Chemical Industrial Products and Daewoo.\",\n",
       " 'He launched the Tata Nano, India’s most affordable car at Rs.',\n",
       " '1 lakh.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c8ebfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ab9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords and filter and then apply Stemming\n",
    "\n",
    "for i in range(len(Sentence)):\n",
    "    words=nltk.word_tokenize(Sentence[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    Sentence[i]=' '.join(words) ## Converting all the word into sentences\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "116e5ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in 1991 , took chairman jrd tata .',\n",
       " \"under chairmanship , compani saw merger like land rover jaguar 's merger tata motor , coru 's merger tata steel , tetley 's merger tata tea , brunner mond , gener chemic industri product daewoo .\",\n",
       " 'he launch tata nano , india ’ afford car rs .',\n",
       " '1 lakh .']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c62908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords and filter and then apply Snowball Stemming\n",
    "\n",
    "stemmm= SnowballStemmer(\"english\")\n",
    "\n",
    "for i in range(len(Sentence)):\n",
    "    words=nltk.word_tokenize(Sentence[i])\n",
    "    words=[stemmm.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    Sentence[i]=' '.join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b91169cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1991 , took chairman jrd tata .',\n",
       " \"chairmanship , compani saw merger like land rover jaguar 's merger tata motor , coru 's merger tata steel , tetley 's merger tata tea , brunner mond , gener chemic industri product daewoo .\",\n",
       " 'launch tata nano , india ’ afford car rs .',\n",
       " '1 lakh .']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30ddcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords and filter and then apply Lemmatization\n",
    "\n",
    "Lemma=WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(Sentence)):\n",
    "    words=nltk.word_tokenize(Sentence[i])\n",
    "    words=[Lemma.lemmatize(word,pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    Sentence[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf4e439f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1991 , take chairman jrd tata .',\n",
       " \"chairmanship , compani saw merger like land rover jaguar 's merger tata motor , coru 's merger tata steel , tetley 's merger tata tea , brunner mond , gener chemic industri product daewoo .\",\n",
       " 'launch tata nano , india ’ afford car rs .',\n",
       " '1 lakh .']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6fb5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "165d00bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f921121",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_filtered=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeff6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    if w  not in Stopwords:\n",
    "        words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9070deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', '1991', ',', 'took', 'chairman', 'JRD', 'Tata', '.', 'Under', 'chairmanship', ',', 'company', 'saw', 'mergers', 'like', 'Land', 'Rover', 'Jaguar', \"'s\", 'merger', 'Tata', 'Motors', ',', 'Corus', \"'s\", 'merger', 'Tata', 'Steel', ',', 'Tetley', \"'s\", 'merger', 'Tata', 'Tea', ',', 'Brunner', 'Mond', ',', 'General', 'Chemical', 'Industrial', 'Products', 'Daewoo', '.', 'He', 'launched', 'Tata', 'Nano', ',', 'India', '’', 'affordable', 'car', 'Rs', '.', '1', 'lakh', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea32681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
